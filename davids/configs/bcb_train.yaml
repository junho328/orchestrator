model_log_name: base_model
model_name_or_path: Qwen/Qwen2.5-7B-Instruct
tokenizer_name_or_path: ${model_name_or_path}

use_peft: true
load_in_4bit: true

lora_r: 16
lora_alpha: 32
lora_dropout: 0.0
lora_target_modules: all-linear 
lora_modules_to_save: all-linear
lora_task_type: "CAUSAL_LM"
use_rslora: false
bnb_4bit_quant_type: "nf4"
use_bnb_nested_quant: false

model_args:
  _target_: trl.trainer.model_config.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  lora_r: ${lora_r}
  lora_alpha: ${lora_alpha}
  lora_dropout: ${lora_dropout}
  lora_target_modules: ${lora_target_modules}
  lora_modules_to_save: ${lora_modules_to_save}
  lora_task_type: ${lora_task_type}
  use_rslora: ${use_rslora}
  bnb_4bit_quant_type: ${bnb_4bit_quant_type}
  use_bnb_nested_quant: ${use_bnb_nested_quant}

tokenizer:
  _target_: transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true

make_tokenizer_fn:
  _target_: orchestrator.hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}

peft_config:
  _target_: trl.trainer.utils.get_peft_config
  model_args: ${model_args}

data_log_name: bigcodebench

# BigCodeBench dataset configuration
dataset_id_or_path: bigcode/bigcodebench
dataset_local_directory: ${dataset_id_or_path}
dataset_configuration: null

# Worker models for orchestration
worker_models:
  - Qwen/Qwen2.5-3B-Instruct
  - microsoft/Phi-3-mini-4k-instruct
  - meta-llama/Llama-3.2-3B-Instruct
  - Qwen/Qwen2.5-Coder-3B-Instruct

max_routing_steps: 5

# Few-shot examples
num_few_shot_examples: 3

make_dataset_fn:
  _target_: orchestrator.data_utils.orchestrator_dataset.OrchestratorDataset
  worker_models: ${worker_models}
  max_routing_steps: ${max_routing_steps}
  split: train
  max_samples: null
  seed: 42
  apply_chat_template: true
  max_prompt_length: 2048
  tokenizer_kwargs: {}
  num_few_shot_examples: ${num_few_shot_examples}

make_eval_dataset_fn:
  _target_: orchestrator.data_utils.orchestrator_dataset.OrchestratorDataset
  worker_models: ${worker_models}
  max_routing_steps: ${max_routing_steps}
  split: test
  max_samples: 100
  seed: 42
  apply_chat_template: true
  max_prompt_length: 2048
  tokenizer_kwargs: {}
  num_few_shot_examples: ${num_few_shot_examples}

