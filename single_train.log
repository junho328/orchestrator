nohup: ignoring input
W1110 17:01:05.377000 1556048 torch/distributed/run.py:774] 
W1110 17:01:05.377000 1556048 torch/distributed/run.py:774] *****************************************
W1110 17:01:05.377000 1556048 torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1110 17:01:05.377000 1556048 torch/distributed/run.py:774] *****************************************
INFO 11-10 17:01:19 [__init__.py:216] Automatically detected platform cuda.
INFO 11-10 17:01:19 [__init__.py:216] Automatically detected platform cuda.
Warning: The cache directory for DeepSpeed Triton autotune, /home/jhna/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/jhna/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Training run_name: None
Training project: huggingface
Training run_name: None
Training project: huggingface
Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]W1110 17:01:33.202000 1556048 torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1110 17:01:33.203000 1556048 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1556348 closing signal SIGTERM
W1110 17:01:33.204000 1556048 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1556349 closing signal SIGTERM
Warning: The cache directory for DeepSpeed Triton autotune, /home/jhna/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Traceback (most recent call last):
  File "/home/jhna/uv_venvs/orchestrator/bin/accelerate", line 10, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/jhna/uv_venvs/orchestrator/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/home/jhna/uv_venvs/orchestrator/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1220, in launch_command
    deepspeed_launcher(args)
  File "/home/jhna/uv_venvs/orchestrator/lib/python3.11/site-packages/accelerate/commands/launch.py", line 906, in deepspeed_launcher
    distrib_run.run(args)
  File "/home/jhna/uv_venvs/orchestrator/lib/python3.11/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/jhna/uv_venvs/orchestrator/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jhna/uv_venvs/orchestrator/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    result = agent.run()
             ^^^^^^^^^^^
  File "/home/jhna/uv_venvs/orchestrator/lib/python3.11/site-packages/torch/distributed/elastic/metrics/api.py", line 138, in wrapper
    result = f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
  File "/home/jhna/uv_venvs/orchestrator/lib/python3.11/site-packages/torch/distributed/elastic/agent/server/api.py", line 715, in run
    result = self._invoke_run(role)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jhna/uv_venvs/orchestrator/lib/python3.11/site-packages/torch/distributed/elastic/agent/server/api.py", line 879, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/jhna/uv_venvs/orchestrator/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 1556048 got signal: 15
Exception ignored in atexit callback: <function matmul_ext_update_autotune_table at 0x7f412f26e020>
Traceback (most recent call last):
  File "/home/jhna/uv_venvs/orchestrator/lib/python3.11/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 480, in matmul_ext_update_autotune_table
    fp16_matmul._update_autotune_table()
  File "/home/jhna/uv_venvs/orchestrator/lib/python3.11/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 457, in _update_autotune_table
    TritonMatmul._update_autotune_table(__class__.__name__ + "_2d_kernel", __class__._2d_kernel)
  File "/home/jhna/uv_venvs/orchestrator/lib/python3.11/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 186, in _update_autotune_table
    cache_manager.put(autotune_table)
  File "/home/jhna/uv_venvs/orchestrator/lib/python3.11/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 103, in put
    with open(self.file_path + ".tmp", 'wb') as handle:
OSError: [Errno 122] Disk quota exceeded
