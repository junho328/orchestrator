defaults:
  - base
  - _self_

trainer_log_name: orchestrator_ppo
logging_prob: 0.1

max_prompt_length: 2048
max_completion_length: 2048

per_device_train_batch_size: 4
train_batch_size: 16
mini_batch_size: 4
ppo_epochs: 3
score_repeats: 1
chunk_size: 16
format_bonus: 0.0

# Worker models configuration (inherited from data_cfg)

# Set Orchestrator rewards
reward_fns:
  _target_: orchestrator.trainers.orchestrator_engine.OrchestratorReward
  worker_models: ${worker_models}
  max_tokens: 2048
  temperature: 0.8
  max_routing_steps: ${max_routing_steps}
  output_dir: ${output_dir}
  coordination_log_dir: ${output_dir}
  chunk_size: ${chunk_size}
  score_repeats: ${score_repeats}
  use_local_models: true
  device_map: auto
  format_bonus: ${format_bonus}

trainer_args:
  _target_: trl.PPOConfig
  output_dir: ${output_dir}
  per_device_train_batch_size: ${per_device_train_batch_size}
  mini_batch_size: ${mini_batch_size}
  ppo_epochs: ${ppo_epochs}
  learning_rate: ${learning_rate}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  seed: ${seed}
  max_prompt_length: ${max_prompt_length}
  max_completion_length: ${max_completion_length}

trainer:
  _target_: orchestrator.trainers.orchestrator_engine.CustomPPOTrainer
  reward_funcs: ${reward_fns}
  args: ${trainer_args}
  logging_prob: ${logging_prob}

